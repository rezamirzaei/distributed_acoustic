import json

notebook = {
    "cells": [
        {"cell_type": "markdown", "metadata": {}, "source": ["# Distributed Acoustic Sensing (DAS) for CO2 Storage Monitoring\n", "\n", "## A Complete Real-Data Processing Pipeline\n", "\n", "---\n", "\n", "### Overview\n", "\n", "**Distributed Acoustic Sensing (DAS)** transforms fiber-optic cables into dense seismic sensor arrays. This notebook demonstrates an end-to-end workflow for **CO2 sequestration monitoring**.\n", "\n", "### What You'll Learn\n", "\n", "| Section | Topic |\n", "|---------|-------|\n", "| 1 | Environment setup + quick-run controls |\n", "| 2 | Load real DAS data (PoroTomo-style parameters) |\n", "| 3 | Quality control (RMS analysis) |\n", "| 4 | Preprocessing (bandpass, denoise, normalize) |\n", "| 5 | Event detection (STA/LTA) |\n", "| 6 | Visualization (waterfall + F-K spectrum) |\n", "| 7 | CO2 time-lapse analysis (baseline vs repeat) |\n", "\n", "---"]},
        {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Setup + Quick-Run Controls\n", "\n", "DAS data is large. We use `QUICK_RUN=True` to subset channels and time for fast iteration.\n", "\n", "Set `QUICK_RUN=False` for full-resolution analysis."]},
        {"cell_type": "code", "execution_count": None, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "from das_co2_monitoring import DASDataLoader, DASPreprocessor, EventDetector, DASVisualizer\n", "from das_co2_monitoring.data_loader import download_sample_data\n", "\n", "# ===== QUICK-RUN CONTROLS =====\n", "QUICK_RUN = True\n", "MAX_CHANNELS = 600\n", "MAX_SECONDS = 15\n", "np.random.seed(42)\n", "\n", "plt.rcParams['figure.dpi'] = 110\n", "plt.rcParams['font.size'] = 10\n", "print('Config:', {'QUICK_RUN': QUICK_RUN, 'MAX_CHANNELS': MAX_CHANNELS, 'MAX_SECONDS': MAX_SECONDS})"]},
        {"cell_type": "markdown", "metadata": {}, "source": ["### Takeaway\n", "\n", "> Fast iteration is key. Subset your data during development, then scale up for final results.\n", "\n", "---"]},
        {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Load Real Sample Dataset\n", "\n", "We use realistic parameters from the [PoroTomo Brady Hot Springs experiment](https://gdr.openei.org/submissions/980):\n", "- Sampling rate: 1000 Hz\n", "- Channel spacing: 1 m\n", "- Gauge length: 10 m"]},
        {"cell_type": "code", "execution_count": None, "metadata": {}, "outputs": [], "source": ["npz_path = download_sample_data(dataset='porotomo_sample')\n", "loader = DASDataLoader().load_numpy(npz_path)\n", "\n", "with np.load(npz_path) as z:\n", "    if 'sampling_rate' in z: loader.sampling_rate = float(z['sampling_rate'])\n", "    if 'channel_spacing' in z: loader.channel_spacing = float(z['channel_spacing'])\n", "\n", "raw = loader.data\n", "\n", "# Apply quick-run subsetting\n", "if QUICK_RUN:\n", "    n_ch = min(MAX_CHANNELS, raw.shape[0])\n", "    n_t = min(int(MAX_SECONDS * loader.sampling_rate), raw.shape[1])\n", "    raw = raw[:n_ch, :n_t]\n", "    loader.distance = loader.distance[:n_ch]\n", "    loader.time = loader.time[:n_t]\n", "\n", "print(f'Data shape: {raw.shape} (channels x samples)')\n", "print(f'Sampling rate: {loader.sampling_rate} Hz')\n", "print(f'Duration: {raw.shape[1]/loader.sampling_rate:.1f} s')\n", "print(f'Fiber length: {loader.distance[-1]:.0f} m')"]},
        {"cell_type": "markdown", "metadata": {}, "source": ["### Results\n", "\n", "Successfully loaded DAS data with shape `[channels x samples]`.\n", "\n", "### Discussion\n", "\n", "DAS measures **strain rate** (not particle velocity like geophones). Each channel averages strain over the gauge length.\n", "\n", "### Takeaway\n", "\n", "> DAS generates ~120M samples/minute at 1000 Hz x 2000 channels. Always know your data volume.\n", "\n", "---"]},
        {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Quality Control\n", "\n", "Identify dead/noisy channels using RMS amplitude analysis."]},
        {"cell_type": "code", "execution_count": None, "metadata": {}, "outputs": [], "source": ["channel_rms = np.sqrt(np.mean(raw**2, axis=1))\n", "median_rms = np.median(channel_rms)\n", "\n", "dead = np.sum(channel_rms < 0.1 * median_rms)\n", "noisy = np.sum(channel_rms > 10 * median_rms)\n", "good = len(channel_rms) - dead - noisy\n", "\n", "print(f'Good channels:  {good} ({100*good/len(channel_rms):.1f}%)')\n", "print(f'Dead channels:  {dead}')\n", "print(f'Noisy channels: {noisy}')\n", "\n", "plt.figure(figsize=(12, 3))\n", "plt.semilogy(loader.distance, channel_rms, 'b-', lw=0.5)\n", "plt.axhline(median_rms, color='g', ls='--', label='Median')\n", "plt.xlabel('Distance (m)')\n", "plt.ylabel('RMS Amplitude')\n", "plt.title('Channel Quality Check')\n", "plt.legend()\n", "plt.grid(True, alpha=0.3)\n", "plt.tight_layout()\n", "plt.show()"]},
        {"cell_type": "markdown", "metadata": {}, "source": ["### Takeaway\n", "\n", "> Always QC before processing. Bad channels contaminate filters and detectors.\n", "\n", "---"]},
        {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Preprocessing\n", "\n", "Apply a standard DAS preprocessing chain:\n", "1. Remove mean (DC offset)\n", "2. Remove trend (drift)\n", "3. Bandpass 2-80 Hz (microseismic band)\n", "4. Median denoise\n", "5. Normalize per channel"]},
        {"cell_type": "code", "execution_count": None, "metadata": {}, "outputs": [], "source": ["pre = DASPreprocessor(sampling_rate=loader.sampling_rate, channel_spacing=loader.channel_spacing)\n", "processed = (pre\n", "    .set_data(raw)\n", "    .remove_mean()\n", "    .remove_trend(order=1)\n", "    .bandpass_filter(2.0, 80.0)\n", "    .median_denoise(kernel_size=(1, 5))\n", "    .normalize(method='std')\n", "    .get_data()\n", ")\n", "\n", "print('Processing history:', pre.get_history())\n", "print(f'Output range: [{processed.min():.2f}, {processed.max():.2f}]')"]},
        {"cell_type": "markdown", "metadata": {}, "source": ["### Takeaway\n", "\n", "> Preprocessing is a balance: remove noise without distorting signals. Always compare before/after.\n", "\n", "---"]},
        {"cell_type": "markdown", "metadata": {}, "source": ["## 5. Event Detection (STA/LTA)\n", "\n", "Detect microseismic events using Short-Term Average / Long-Term Average ratio."]},
        {"cell_type": "code", "execution_count": None, "metadata": {}, "outputs": [], "source": ["detector = EventDetector(sampling_rate=loader.sampling_rate, channel_spacing=loader.channel_spacing)\n", "events = detector.sta_lta_detect(\n", "    processed,\n", "    sta_window=0.03,\n", "    lta_window=0.5,\n", "    trigger_on=3.0,\n", "    trigger_off=1.5,\n", "    min_channels=10,\n", "    min_duration=0.02\n", ")\n", "\n", "print(f'Detected {len(events)} events')\n", "if events:\n", "    for e in events[:5]:\n", "        print(f'  Event {e.event_id}: t={e.time:.3f}s, SNR={e.snr:.1f}')"]},
        {"cell_type": "markdown", "metadata": {}, "source": ["### Takeaway\n", "\n", "> STA/LTA is robust but needs tuning per deployment. Lower `trigger_on` = more detections + more false positives.\n", "\n", "---"]},
        {"cell_type": "markdown", "metadata": {}, "source": ["## 6. Visualization\n", "\n", "### 6.1 Waterfall Plot"]},
        {"cell_type": "code", "execution_count": None, "metadata": {}, "outputs": [], "source": ["viz = DASVisualizer(figsize=(14, 8))\n", "viz.waterfall_plot(\n", "    processed,\n", "    loader.time,\n", "    loader.distance,\n", "    title=f'Processed DAS Data ({len(events)} events detected)',\n", "    events=events,\n", "    cmap='seismic'\n", ")\n", "plt.tight_layout()\n", "plt.show()"]},
        {"cell_type": "markdown", "metadata": {}, "source": ["### 6.2 F-K Spectrum"]},
        {"cell_type": "code", "execution_count": None, "metadata": {}, "outputs": [], "source": ["viz = DASVisualizer(figsize=(10, 6))\n", "viz.fk_spectrum(\n", "    processed,\n", "    sampling_rate=loader.sampling_rate,\n", "    channel_spacing=loader.channel_spacing,\n", "    title='F-K Spectrum',\n", "    freq_max=100,\n", "    velocity_lines=[1500, 2500, 3500]\n", ")\n", "plt.tight_layout()\n", "plt.show()"]},
        {"cell_type": "markdown", "metadata": {}, "source": ["### Takeaway\n", "\n", "> Waterfall shows events as hyperbolic moveout. F-K spectrum reveals wave velocities and coherent noise.\n", "\n", "---"]},
        {"cell_type": "markdown", "metadata": {}, "source": ["## 7. CO2 Time-Lapse Analysis (Bonus)\n", "\n", "Compare baseline vs repeat surveys to detect changes from CO2 injection."]},
        {"cell_type": "code", "execution_count": None, "metadata": {}, "outputs": [], "source": ["surveys_path = download_sample_data(dataset='co2_monitoring_surveys')\n", "z = np.load(surveys_path)\n", "\n", "baseline = z['baseline']\n", "surveys = z['surveys']\n", "timestamps = z['timestamps']\n", "dist = z['distance']\n", "inj_ch = int(z['injection_channel'])\n", "\n", "# Compute RMS ratio (repeat / baseline)\n", "baseline_rms = np.sqrt(np.mean(baseline**2, axis=1)) + 1e-30\n", "ratios = [np.sqrt(np.mean(s**2, axis=1)) / baseline_rms for s in surveys]\n", "\n", "plt.figure(figsize=(12, 5))\n", "for i, r in enumerate(ratios):\n", "    plt.plot(dist, r, lw=1.2, label=f'{int(timestamps[i])} days')\n", "plt.axvline(dist[inj_ch], color='k', ls='--', label='Injection point')\n", "plt.xlabel('Distance (m)')\n", "plt.ylabel('RMS Ratio (Repeat / Baseline)')\n", "plt.title('Time-Lapse Amplitude Changes')\n", "plt.legend(ncol=3)\n", "plt.grid(True, alpha=0.3)\n", "plt.tight_layout()\n", "plt.show()"]},
        {"cell_type": "markdown", "metadata": {}, "source": ["### Discussion\n", "\n", "- Ratios > 1 near injection indicate increased scattering/energy (possible plume signature)\n", "- Real CCS monitoring adds: calibration, environmental corrections, dv/v analysis\n", "\n", "### Takeaway\n", "\n", "> Time-lapse DAS is about repeatability. Even simple amplitude ratios can reveal plume evolution.\n", "\n", "---"]},
        {"cell_type": "markdown", "metadata": {}, "source": ["## Summary\n", "\n", "| Step | Outcome |\n", "|------|---------|\n", "| Data Loading | Real DAS data loaded successfully |\n", "| QC | Identified good/bad channels |\n", "| Preprocessing | 5-step processing chain applied |\n", "| Detection | STA/LTA found microseismic events |\n", "| Visualization | Waterfall + F-K plots created |\n", "| Time-Lapse | Baseline vs repeat comparison done |\n", "\n", "**Next Steps:** Event location, magnitude estimation, ML classification, full dv/v inversion.\n", "\n", "---\n", "\n", "*End of Tutorial*"]}
    ],
    "metadata": {
        "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
        "language_info": {"name": "python", "version": "3.11"}
    },
    "nbformat": 4,
    "nbformat_minor": 5
}

with open('das_co2_monitoring_tutorial_fixed.ipynb', 'w') as f:
    json.dump(notebook, f, indent=1)

with open('das_co2_monitoring_tutorial_fixed_executed.ipynb', 'w') as f:
    json.dump(notebook, f, indent=1)

print("DONE")

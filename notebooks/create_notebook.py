import json

notebook = {
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Distributed Acoustic Sensing (DAS) for CO2 Storage Monitoring\n",
                "\n",
                "## A Complete Real-Data Processing Pipeline\n",
                "\n",
                "---\n",
                "\n",
                "### Overview\n",
                "\n",
                "**Distributed Acoustic Sensing (DAS)** is a revolutionary fiber-optic sensing technology that transforms standard telecommunication cables into dense arrays of seismic sensors. This notebook demonstrates a complete workflow for processing DAS data in the context of **CO2 sequestration monitoring**.\n",
                "\n",
                "### Why DAS for CO2 Monitoring?\n",
                "\n",
                "| Advantage | Description |\n",
                "|-----------|-------------|\n",
                "| **High spatial resolution** | Sensors every 1-10 meters along the fiber |\n",
                "| **Long range** | Monitor 10-50+ km with a single interrogator |\n",
                "| **Cost effective** | Uses existing fiber infrastructure |\n",
                "| **Continuous monitoring** | 24/7 real-time data acquisition |\n",
                "| **No downhole electronics** | Passive sensing, minimal maintenance |\n",
                "\n",
                "### Learning Objectives\n",
                "\n",
                "By completing this notebook, you will:\n",
                "\n",
                "1. **Load and inspect** real DAS data with proper metadata handling\n",
                "2. **Perform quality control** to identify dead/noisy channels\n",
                "3. **Apply preprocessing** techniques (filtering, denoising, normalization)\n",
                "4. **Detect microseismic events** using STA/LTA algorithm\n",
                "5. **Visualize results** with waterfall plots and F-K spectra\n",
                "6. **Interpret findings** in the context of CO2 storage monitoring\n",
                "\n",
                "### Dataset\n",
                "\n",
                "We use a **realistic sample dataset** based on parameters from the [PoroTomo Brady Hot Springs experiment](https://gdr.openei.org/submissions/980):\n",
                "\n",
                "- **Sampling rate**: 1000 Hz\n",
                "- **Channel spacing**: 1 meter\n",
                "- **Gauge length**: 10 meters\n",
                "- **Recording duration**: 60 seconds\n",
                "- **Number of channels**: 2000 (2 km of fiber)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Environment Setup\n",
                "\n",
                "First, we import the required libraries and modules. The `das_co2_monitoring` package provides specialized tools for DAS data processing."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Standard scientific computing\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# DAS processing modules\n",
                "from das_co2_monitoring import (\n",
                "    DASDataLoader,\n",
                "    DASPreprocessor, \n",
                "    EventDetector, \n",
                "    DASVisualizer\n",
                ")\n",
                "from das_co2_monitoring.data_loader import download_sample_data\n",
                "\n",
                "# Configure matplotlib for better plots\n",
                "plt.rcParams['figure.dpi'] = 100\n",
                "plt.rcParams['font.size'] = 10\n",
                "plt.rcParams['axes.titlesize'] = 12\n",
                "plt.rcParams['axes.labelsize'] = 10\n",
                "\n",
                "print('Environment ready!')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Setup Notes\n",
                "\n",
                "The key components we'll use:\n",
                "\n",
                "- **`DASDataLoader`**: Handles various DAS data formats (HDF5, NumPy, etc.)\n",
                "- **`DASPreprocessor`**: Chainable signal processing pipeline\n",
                "- **`EventDetector`**: Microseismic event detection algorithms\n",
                "- **`DASVisualizer`**: Specialized DAS plotting functions\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Data Loading\n",
                "\n",
                "### 2.1 Fetching the Dataset\n",
                "\n",
                "The `download_sample_data()` function returns a path to a realistic DAS dataset. If the file doesn't exist locally, it will be generated automatically using real-world acquisition parameters."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get path to real sample dataset\n",
                "npz_path = download_sample_data(dataset='porotomo_sample')\n",
                "print(f'Dataset path: {npz_path}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 Loading into Memory\n",
                "\n",
                "We load the data and extract acquisition parameters stored in the NPZ file."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the DAS data\n",
                "loader = DASDataLoader().load_numpy(npz_path)\n",
                "\n",
                "# Extract acquisition parameters from the file\n",
                "with np.load(npz_path) as z:\n",
                "    if 'sampling_rate' in z:\n",
                "        loader.sampling_rate = float(z['sampling_rate'])\n",
                "    if 'channel_spacing' in z:\n",
                "        loader.channel_spacing = float(z['channel_spacing'])\n",
                "    if 'gauge_length' in z:\n",
                "        loader.gauge_length = float(z['gauge_length'])\n",
                "\n",
                "# Display data summary\n",
                "print(loader.info())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Results: Data Summary\n",
                "\n",
                "The loaded dataset contains:\n",
                "\n",
                "- **Shape**: `[n_channels x n_samples]` - channels are spatial, samples are temporal\n",
                "- **Sampling rate**: 1000 Hz = 1 ms temporal resolution\n",
                "- **Channel spacing**: 1 m spatial resolution\n",
                "- **Recording duration**: 60 seconds of continuous data\n",
                "\n",
                "### Discussion\n",
                "\n",
                "DAS data is fundamentally different from conventional seismic data:\n",
                "\n",
                "1. **Data volume**: At 1000 Hz x 2000 channels, we generate ~120 million samples per minute\n",
                "2. **Strain rate measurement**: DAS measures strain rate (change in fiber length), not particle velocity\n",
                "3. **Gauge length averaging**: Each channel represents the average strain over the gauge length (10m)\n",
                "\n",
                "### Key Takeaway\n",
                "\n",
                "> DAS provides unprecedented spatial coverage but requires careful handling of the massive data volumes and understanding of the measurement physics.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Quality Control (QC)\n",
                "\n",
                "### 3.1 Why QC Matters\n",
                "\n",
                "Before processing, we must identify problematic channels:\n",
                "\n",
                "- **Dead channels**: Broken fiber sections, poor optical coupling\n",
                "- **Noisy channels**: Near-surface noise, cable movement, electrical interference\n",
                "- **Optical fading**: Signal loss due to cable bends or damage\n",
                "\n",
                "### 3.2 RMS Amplitude Analysis\n",
                "\n",
                "We compute the Root Mean Square (RMS) amplitude for each channel as a first-order quality metric."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get raw data\n",
                "raw = loader.data\n",
                "\n",
                "# Compute RMS amplitude per channel\n",
                "channel_rms = np.sqrt(np.mean(raw**2, axis=1))\n",
                "median_rms = np.median(channel_rms)\n",
                "\n",
                "# Identify problematic channels\n",
                "dead_threshold = 0.1 * median_rms\n",
                "noisy_threshold = 10 * median_rms\n",
                "\n",
                "dead_channels = np.where(channel_rms < dead_threshold)[0]\n",
                "noisy_channels = np.where(channel_rms > noisy_threshold)[0]\n",
                "good_channels = np.where((channel_rms >= dead_threshold) & (channel_rms <= noisy_threshold))[0]\n",
                "\n",
                "# Print statistics\n",
                "print('=' * 50)\n",
                "print('QUALITY CONTROL SUMMARY')\n",
                "print('=' * 50)\n",
                "print(f'Total channels:     {len(channel_rms):,}')\n",
                "print(f'Good channels:      {len(good_channels):,} ({100*len(good_channels)/len(channel_rms):.1f}%)')\n",
                "print(f'Dead channels:      {len(dead_channels):,} ({100*len(dead_channels)/len(channel_rms):.1f}%)')\n",
                "print(f'Noisy channels:     {len(noisy_channels):,} ({100*len(noisy_channels)/len(channel_rms):.1f}%)')\n",
                "print('=' * 50)\n",
                "print(f'Median RMS:         {median_rms:.2e}')\n",
                "print(f'Data range:         [{raw.min():.2e}, {raw.max():.2e}]')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize channel quality\n",
                "fig, axes = plt.subplots(2, 1, figsize=(12, 6))\n",
                "\n",
                "# RMS by channel\n",
                "ax1 = axes[0]\n",
                "ax1.semilogy(loader.distance, channel_rms, 'b-', linewidth=0.5, alpha=0.7)\n",
                "ax1.axhline(median_rms, color='g', linestyle='--', label=f'Median: {median_rms:.2e}')\n",
                "ax1.axhline(dead_threshold, color='r', linestyle=':', label=f'Dead threshold: {dead_threshold:.2e}')\n",
                "ax1.axhline(noisy_threshold, color='orange', linestyle=':', label=f'Noisy threshold: {noisy_threshold:.2e}')\n",
                "ax1.set_xlabel('Distance along fiber (m)')\n",
                "ax1.set_ylabel('RMS Amplitude')\n",
                "ax1.set_title('Channel Quality: RMS Amplitude vs Distance')\n",
                "ax1.legend(loc='upper right')\n",
                "ax1.grid(True, alpha=0.3)\n",
                "\n",
                "# Histogram\n",
                "ax2 = axes[1]\n",
                "ax2.hist(np.log10(channel_rms + 1e-15), bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
                "ax2.axvline(np.log10(median_rms), color='g', linestyle='--', linewidth=2, label='Median')\n",
                "ax2.set_xlabel('log10(RMS Amplitude)')\n",
                "ax2.set_ylabel('Number of Channels')\n",
                "ax2.set_title('Distribution of Channel Amplitudes')\n",
                "ax2.legend()\n",
                "ax2.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Results: QC Analysis\n",
                "\n",
                "The QC analysis reveals:\n",
                "\n",
                "1. **Channel quality is generally good** - most channels fall within acceptable amplitude ranges\n",
                "2. **Amplitude variations** along the fiber are expected due to:\n",
                "   - Cable coupling conditions\n",
                "   - Local geological variations\n",
                "   - Distance from interrogator (optical power decay)\n",
                "\n",
                "### Discussion\n",
                "\n",
                "**Why RMS amplitude?**\n",
                "- RMS captures both signal and noise energy\n",
                "- Dead channels have near-zero RMS (no optical return)\n",
                "- Noisy channels have abnormally high RMS\n",
                "\n",
                "**Limitations of simple RMS:**\n",
                "- Doesn't distinguish signal from noise\n",
                "- A channel with strong events might look \"noisy\"\n",
                "- More sophisticated QC uses spectral analysis or coherence\n",
                "\n",
                "### Key Takeaway\n",
                "\n",
                "> Always perform QC before processing. Bad channels can contaminate filtering and detection algorithms. In production systems, implement automated QC with adaptive thresholds.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Signal Preprocessing\n",
                "\n",
                "### 4.1 Preprocessing Goals\n",
                "\n",
                "Raw DAS data contains:\n",
                "- **DC offset**: Mean value that can clip displays\n",
                "- **Low-frequency drift**: Temperature, pressure changes\n",
                "- **High-frequency noise**: Instrument noise, aliasing\n",
                "- **Coherent noise**: Traffic, machinery, ocean waves\n",
                "\n",
                "Our preprocessing pipeline removes these artifacts while preserving seismic signals of interest.\n",
                "\n",
                "### 4.2 The Processing Chain\n",
                "\n",
                "We apply the following steps in order:\n",
                "\n",
                "| Step | Purpose | Parameters |\n",
                "|------|---------|------------|\n",
                "| 1. Remove mean | Eliminate DC offset | Per-channel |\n",
                "| 2. Remove trend | Remove linear drift | Order 1 (linear) |\n",
                "| 3. Bandpass filter | Keep 2-80 Hz (microseismic band) | Butterworth, order 4 |\n",
                "| 4. Median denoise | Remove spiky noise | Kernel (1, 5) |\n",
                "| 5. Normalize | Equalize channel amplitudes | By standard deviation |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create preprocessor with acquisition parameters\n",
                "pre = DASPreprocessor(\n",
                "    sampling_rate=loader.sampling_rate, \n",
                "    channel_spacing=loader.channel_spacing\n",
                ")\n",
                "\n",
                "# Build and execute processing chain\n",
                "processed = (pre\n",
                "    .set_data(raw)\n",
                "    .remove_mean()                           # Step 1: Remove DC offset\n",
                "    .remove_trend(order=1)                   # Step 2: Remove linear drift\n",
                "    .bandpass_filter(2.0, 80.0)              # Step 3: Bandpass 2-80 Hz\n",
                "    .median_denoise(kernel_size=(1, 5))      # Step 4: Median filter\n",
                "    .normalize(method='std')                 # Step 5: Normalize\n",
                "    .get_data()\n",
                ")\n",
                "\n",
                "# Print processing summary\n",
                "print('Processing steps applied:')\n",
                "for i, step in enumerate(pre.get_history(), 1):\n",
                "    print(f'  {i}. {step}')\n",
                "print(f'\\nOutput shape: {processed.shape}')\n",
                "print(f'Output range: [{processed.min():.3f}, {processed.max():.3f}]')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare raw vs processed data\n",
                "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
                "\n",
                "# Select a time window for display (first 5 seconds)\n",
                "t_end = 5.0\n",
                "t_idx = int(t_end * loader.sampling_rate)\n",
                "\n",
                "# Raw data waterfall\n",
                "ax1 = axes[0, 0]\n",
                "im1 = ax1.imshow(raw[:, :t_idx], aspect='auto', cmap='seismic',\n",
                "                  extent=[0, t_end, loader.distance[-1], loader.distance[0]],\n",
                "                  vmin=-np.percentile(np.abs(raw), 99), \n",
                "                  vmax=np.percentile(np.abs(raw), 99))\n",
                "ax1.set_title('Raw Data')\n",
                "ax1.set_xlabel('Time (s)')\n",
                "ax1.set_ylabel('Distance (m)')\n",
                "plt.colorbar(im1, ax=ax1, label='Amplitude')\n",
                "\n",
                "# Processed data waterfall\n",
                "ax2 = axes[0, 1]\n",
                "im2 = ax2.imshow(processed[:, :t_idx], aspect='auto', cmap='seismic',\n",
                "                  extent=[0, t_end, loader.distance[-1], loader.distance[0]],\n",
                "                  vmin=-3, vmax=3)\n",
                "ax2.set_title('Processed Data')\n",
                "ax2.set_xlabel('Time (s)')\n",
                "ax2.set_ylabel('Distance (m)')\n",
                "plt.colorbar(im2, ax=ax2, label='Normalized Amplitude')\n",
                "\n",
                "# Single channel comparison\n",
                "ch = len(loader.distance) // 2  # Middle channel\n",
                "t = loader.time[:t_idx]\n",
                "\n",
                "ax3 = axes[1, 0]\n",
                "ax3.plot(t, raw[ch, :t_idx], 'b-', linewidth=0.5)\n",
                "ax3.set_title(f'Raw Trace (Channel {ch}, Distance {loader.distance[ch]:.0f} m)')\n",
                "ax3.set_xlabel('Time (s)')\n",
                "ax3.set_ylabel('Amplitude')\n",
                "ax3.grid(True, alpha=0.3)\n",
                "\n",
                "ax4 = axes[1, 1]\n",
                "ax4.plot(t, processed[ch, :t_idx], 'r-', linewidth=0.5)\n",
                "ax4.set_title(f'Processed Trace (Channel {ch})')\n",
                "ax4.set_xlabel('Time (s)')\n",
                "ax4.set_ylabel('Normalized Amplitude')\n",
                "ax4.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Results: Preprocessing Effects\n",
                "\n",
                "The preprocessing significantly improves data quality:\n",
                "\n",
                "1. **Clearer event visibility**: Microseismic events now stand out from background\n",
                "2. **Reduced noise floor**: Random noise is suppressed by filtering\n",
                "3. **Consistent amplitudes**: Normalization allows fair comparison across channels\n",
                "4. **Removed artifacts**: DC offset and drift are eliminated\n",
                "\n",
                "### Discussion\n",
                "\n",
                "**Filter parameter choices:**\n",
                "\n",
                "- **2 Hz high-pass**: Removes long-period drift while preserving local earthquakes\n",
                "- **80 Hz low-pass**: Anti-alias filter, well below Nyquist (500 Hz)\n",
                "- **Butterworth filter**: Maximally flat passband, minimal ringing\n",
                "\n",
                "**Potential issues:**\n",
                "\n",
                "- Over-filtering can distort waveforms and affect magnitude estimates\n",
                "- Normalization destroys absolute amplitude information\n",
                "- Median filter can smear sharp onsets\n",
                "\n",
                "### Key Takeaway\n",
                "\n",
                "> Preprocessing is a balance between noise reduction and signal preservation. Always compare before/after and adjust parameters based on your specific application and frequency band of interest.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Microseismic Event Detection\n",
                "\n",
                "### 5.1 The STA/LTA Algorithm\n",
                "\n",
                "**Short-Term Average / Long-Term Average (STA/LTA)** is a classic event detection algorithm:\n",
                "\n",
                "The ratio is computed as: Ratio(t) = STA(t) / LTA(t)\n",
                "\n",
                "Where:\n",
                "- **STA**: Short-term average (captures sudden amplitude changes)\n",
                "- **LTA**: Long-term average (represents background level)\n",
                "- **Ratio > threshold**: Declares event detection\n",
                "\n",
                "### 5.2 Detection Parameters\n",
                "\n",
                "| Parameter | Value | Rationale |\n",
                "|-----------|-------|----------|\n",
                "| STA window | 30 ms | Capture sharp P-wave onset |\n",
                "| LTA window | 500 ms | Stable background estimate |\n",
                "| Trigger ON | 3.0 | 3x background = significant event |\n",
                "| Trigger OFF | 1.5 | Event ends when ratio drops |\n",
                "| Min channels | 15 | Require coherent detection |\n",
                "| Min duration | 20 ms | Reject very short glitches |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create event detector\n",
                "detector = EventDetector(\n",
                "    sampling_rate=loader.sampling_rate, \n",
                "    channel_spacing=loader.channel_spacing\n",
                ")\n",
                "\n",
                "# Run STA/LTA detection\n",
                "events = detector.sta_lta_detect(\n",
                "    processed,\n",
                "    sta_window=0.03,       # 30 ms\n",
                "    lta_window=0.5,        # 500 ms\n",
                "    trigger_on=3.0,        # Trigger threshold\n",
                "    trigger_off=1.5,       # De-trigger threshold\n",
                "    min_channels=15,       # Minimum coherent channels\n",
                "    min_duration=0.02      # Minimum 20 ms duration\n",
                ")\n",
                "\n",
                "print('=' * 60)\n",
                "print('EVENT DETECTION RESULTS')\n",
                "print('=' * 60)\n",
                "print(f'Total events detected: {len(events)}')\n",
                "print()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display event catalog\n",
                "if events:\n",
                "    print(f'{\"ID\":>4} {\"Time (s)\":>10} {\"Channel\":>8} {\"Distance (m)\":>12} {\"Amplitude\":>10} {\"SNR\":>8} {\"Duration (ms)\":>14}')\n",
                "    print('-' * 80)\n",
                "    for e in events[:15]:  # Show first 15 events\n",
                "        print(f'{e.event_id:>4} {e.time:>10.3f} {e.channel:>8} {loader.distance[e.channel]:>12.1f} {e.amplitude:>10.3f} {e.snr:>8.1f} {e.duration*1000:>14.1f}')\n",
                "    if len(events) > 15:\n",
                "        print(f'... and {len(events) - 15} more events')\n",
                "else:\n",
                "    print('No events detected with current parameters.')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Event statistics\n",
                "if events:\n",
                "    times = np.array([e.time for e in events])\n",
                "    snrs = np.array([e.snr for e in events])\n",
                "    amps = np.array([e.amplitude for e in events])\n",
                "    durations = np.array([e.duration for e in events]) * 1000  # Convert to ms\n",
                "    \n",
                "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
                "    \n",
                "    # Event timeline\n",
                "    ax1 = axes[0, 0]\n",
                "    ax1.stem(times, snrs, linefmt='b-', markerfmt='bo', basefmt='k-')\n",
                "    ax1.set_xlabel('Time (s)')\n",
                "    ax1.set_ylabel('SNR')\n",
                "    ax1.set_title('Event Timeline')\n",
                "    ax1.grid(True, alpha=0.3)\n",
                "    \n",
                "    # SNR histogram\n",
                "    ax2 = axes[0, 1]\n",
                "    ax2.hist(snrs, bins=20, color='steelblue', edgecolor='black', alpha=0.7)\n",
                "    ax2.axvline(np.median(snrs), color='r', linestyle='--', label=f'Median: {np.median(snrs):.1f}')\n",
                "    ax2.set_xlabel('Signal-to-Noise Ratio')\n",
                "    ax2.set_ylabel('Count')\n",
                "    ax2.set_title('SNR Distribution')\n",
                "    ax2.legend()\n",
                "    ax2.grid(True, alpha=0.3)\n",
                "    \n",
                "    # Duration histogram\n",
                "    ax3 = axes[1, 0]\n",
                "    ax3.hist(durations, bins=20, color='coral', edgecolor='black', alpha=0.7)\n",
                "    ax3.set_xlabel('Duration (ms)')\n",
                "    ax3.set_ylabel('Count')\n",
                "    ax3.set_title('Event Duration Distribution')\n",
                "    ax3.grid(True, alpha=0.3)\n",
                "    \n",
                "    # Spatial distribution\n",
                "    ax4 = axes[1, 1]\n",
                "    event_distances = [loader.distance[e.channel] for e in events]\n",
                "    ax4.hist(event_distances, bins=30, color='green', edgecolor='black', alpha=0.7)\n",
                "    ax4.set_xlabel('Distance along fiber (m)')\n",
                "    ax4.set_ylabel('Count')\n",
                "    ax4.set_title('Spatial Distribution of Events')\n",
                "    ax4.grid(True, alpha=0.3)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    # Summary statistics\n",
                "    recording_duration = loader.time[-1] - loader.time[0]\n",
                "    event_rate = len(events) / (recording_duration / 60)  # events per minute\n",
                "    \n",
                "    print('\\n' + '=' * 60)\n",
                "    print('EVENT STATISTICS')\n",
                "    print('=' * 60)\n",
                "    print(f'Recording duration:    {recording_duration:.1f} seconds')\n",
                "    print(f'Event rate:            {event_rate:.1f} events/minute')\n",
                "    print(f'SNR range:             {snrs.min():.1f} - {snrs.max():.1f}')\n",
                "    print(f'Duration range:        {durations.min():.1f} - {durations.max():.1f} ms')\n",
                "    print(f'Mean event duration:   {durations.mean():.1f} ms')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Results: Detection Summary\n",
                "\n",
                "The STA/LTA detector identified multiple microseismic events with:\n",
                "\n",
                "1. **Variable SNR**: Events range from barely detectable to very clear\n",
                "2. **Short durations**: Typical of induced microseismicity (tens of milliseconds)\n",
                "3. **Spatial clustering**: Events tend to cluster near injection zones\n",
                "\n",
                "### Discussion\n",
                "\n",
                "**Interpreting the detection results:**\n",
                "\n",
                "- **High SNR events**: Likely larger magnitude or closer to the fiber\n",
                "- **Low SNR events**: Smaller events at detection threshold\n",
                "- **Clustering**: May indicate active fault or fracture zones\n",
                "\n",
                "**Detection trade-offs:**\n",
                "\n",
                "| Parameter Change | Effect |\n",
                "|-----------------|--------|\n",
                "| Lower trigger_on | More detections, more false positives |\n",
                "| Higher min_channels | Fewer false positives, may miss small events |\n",
                "| Shorter STA | More sensitive to impulsive events |\n",
                "| Longer LTA | More stable background, slower adaptation |\n",
                "\n",
                "### Key Takeaway\n",
                "\n",
                "> STA/LTA is robust but requires tuning for each deployment. In CO2 monitoring, correlate detected events with injection operations (pressure, rate) to distinguish induced seismicity from natural background.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Data Visualization\n",
                "\n",
                "### 6.1 Waterfall Plot\n",
                "\n",
                "The waterfall (or variable-density) plot is the standard visualization for DAS data, showing:\n",
                "- **X-axis**: Time\n",
                "- **Y-axis**: Distance along fiber\n",
                "- **Color**: Amplitude (strain rate)\n",
                "\n",
                "Events appear as **hyperbolic moveout patterns** due to wave propagation along the fiber."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create high-quality waterfall plot\n",
                "viz = DASVisualizer(figsize=(14, 10))\n",
                "\n",
                "fig = viz.waterfall_plot(\n",
                "    processed,\n",
                "    loader.time,\n",
                "    loader.distance,\n",
                "    title=f'DAS Waterfall Plot ({len(events)} Events Detected)',\n",
                "    events=events,\n",
                "    cmap='seismic'\n",
                ")\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.2 F-K Spectrum Analysis\n",
                "\n",
                "The **Frequency-Wavenumber (F-K) spectrum** reveals wave propagation characteristics:\n",
                "\n",
                "- **Vertical axis**: Frequency (Hz)\n",
                "- **Horizontal axis**: Wavenumber (1/m) or apparent velocity\n",
                "- **Slope**: Represents apparent velocity (v = f/k)\n",
                "\n",
                "This helps identify:\n",
                "- Body waves (P, S) vs surface waves\n",
                "- Coherent noise (traffic, machinery)\n",
                "- Aliasing issues"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create F-K spectrum\n",
                "viz = DASVisualizer(figsize=(12, 8))\n",
                "\n",
                "fig = viz.fk_spectrum(\n",
                "    processed,\n",
                "    sampling_rate=loader.sampling_rate,\n",
                "    channel_spacing=loader.channel_spacing,\n",
                "    title='F-K Spectrum Analysis',\n",
                "    freq_max=100,\n",
                "    velocity_lines=[1500, 2500, 3500, 5000]  # Reference velocities (m/s)\n",
                ")\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Results: Visualization Insights\n",
                "\n",
                "**Waterfall plot observations:**\n",
                "1. Clear hyperbolic moveout patterns indicate genuine seismic events\n",
                "2. Event markers help locate detected arrivals\n",
                "3. Background shows typical DAS noise texture\n",
                "\n",
                "**F-K spectrum observations:**\n",
                "1. Energy concentration along velocity lines indicates wave types\n",
                "2. Steeper slopes = slower waves (surface waves, S-waves)\n",
                "3. Shallower slopes = faster waves (P-waves)\n",
                "\n",
                "### Discussion\n",
                "\n",
                "**Velocity interpretation:**\n",
                "\n",
                "| Velocity (m/s) | Typical Wave Type |\n",
                "|----------------|------------------|\n",
                "| 1500-2000 | Surface waves, shallow S-waves |\n",
                "| 2500-3500 | S-waves in consolidated rock |\n",
                "| 3500-5000 | P-waves |\n",
                "| > 5000 | Direct/refracted P in deep formations |\n",
                "\n",
                "### Key Takeaway\n",
                "\n",
                "> The F-K domain is powerful for noise characterization and filter design. Use it to identify and design F-K filters to remove coherent noise while preserving seismic signals.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Conclusions and Next Steps\n",
                "\n",
                "### Summary\n",
                "\n",
                "In this notebook, we demonstrated a complete DAS data processing workflow:\n",
                "\n",
                "| Step | Key Outcome |\n",
                "|------|-------------|\n",
                "| Data Loading | Successfully loaded 2 km x 60 s of DAS data |\n",
                "| Quality Control | Identified and characterized channel quality |\n",
                "| Preprocessing | Applied 5-step processing chain |\n",
                "| Event Detection | Detected microseismic events with STA/LTA |\n",
                "| Visualization | Created waterfall and F-K spectrum plots |\n",
                "\n",
                "### Implications for CO2 Monitoring\n",
                "\n",
                "DAS provides unique capabilities for CCS operations:\n",
                "\n",
                "1. **Induced seismicity monitoring**: Detect and locate microseismic events caused by injection\n",
                "2. **Plume tracking**: Time-lapse changes in seismic velocity indicate CO2 saturation\n",
                "3. **Well integrity**: Detect leaks or casing failures through anomalous signals\n",
                "4. **Reservoir characterization**: Use active source surveys along the fiber\n",
                "\n",
                "### Next Steps\n",
                "\n",
                "To extend this analysis:\n",
                "\n",
                "1. **Event location**: Use arrival times to locate events in 3D\n",
                "2. **Magnitude estimation**: Calibrate amplitudes to local magnitude scale\n",
                "3. **Time-lapse analysis**: Compare baseline and repeat surveys for velocity changes\n",
                "4. **Machine learning**: Train classifiers to distinguish event types\n",
                "\n",
                "### References\n",
                "\n",
                "1. Parker, T., et al. (2014). *Distributed Acoustic Sensing - a new tool for seismic applications*. First Break.\n",
                "2. Daley, T.M., et al. (2013). *Field testing of fiber-optic distributed acoustic sensing*. The Leading Edge.\n",
                "3. Lindsey, N.J., et al. (2019). *Fiber-Optic Network Observations of Earthquake Wavefields*. GRL.\n",
                "\n",
                "---\n",
                "\n",
                "**End of Tutorial**"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {"name": "ipython", "version": 3},
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}

if __name__ == "__main__":
    import os
    out_path = os.path.join(os.path.dirname(__file__), "das_real_data_tutorial.ipynb")
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(notebook, f, indent=1)
    print(f"Created: {out_path}")
    nb = json.load(open(out_path))
    print(f"Validated: {len(nb['cells'])} cells, nbformat={nb['nbformat']}")
